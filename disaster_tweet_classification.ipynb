{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samuel.lausten/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "print(train_data.head())\n",
    "\n",
    "# Display the first few rows of the test dataset\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7324894514767932\n",
      "Precision: 0.8097014925373134\n",
      "Recall: 0.6687211093990755\n",
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)  # Remove special characters\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "train_data['cleaned_text'] = train_data['text'].apply(clean_text)\n",
    "test_data['cleaned_text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.9, min_df=2, ngram_range=(1, 2))\n",
    "X = tfidf_vectorizer.fit_transform(train_data['cleaned_text'])\n",
    "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
    "\n",
    "# Target variable\n",
    "y = train_data['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model as a baseline\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation F1 Score: 0.6758\n",
      "Epoch 2/5, Validation F1 Score: 0.7521\n",
      "Epoch 3/5, Validation F1 Score: 0.7465\n",
      "Epoch 4/5, Validation F1 Score: 0.7403\n",
      "Epoch 5/5, Validation F1 Score: 0.7351\n",
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)  # Remove special characters\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "train_data['cleaned_text'] = train_data['text'].apply(clean_text)\n",
    "test_data['cleaned_text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenization and Padding\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(train_data['cleaned_text']).toarray()\n",
    "X_test = vectorizer.transform(test_data['cleaned_text']).toarray()\n",
    "\n",
    "# Target variable\n",
    "y = train_data['target'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors and reshape to (batch_size, sequence_length, input_size)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate model, define loss function and optimizer\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch.float())\n",
    "            val_loss += loss.item()\n",
    "            y_val_pred.extend(outputs.squeeze().cpu().numpy())\n",
    "    \n",
    "    y_val_pred = np.array(y_val_pred) > 0.5\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = (test_outputs.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})\n",
    "submission.to_csv('submission_2.csv', index=False)\n",
    "print(\"Submission file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/samuel.lausten/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/samuel.lausten/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Average Training Loss: 0.1492242187388691\n",
      "Average Validation Loss: 0.1208769068083105\n",
      "Validation F1 Score: 0.8057097541633624\n",
      "Epoch 2/3\n",
      "Average Training Loss: 0.10291042138047497\n",
      "Average Validation Loss: 0.1239796940353699\n",
      "Validation F1 Score: 0.807367613200307\n",
      "Epoch 3/3\n",
      "Average Training Loss: 0.07119621112123817\n",
      "Average Validation Loss: 0.13024587673135102\n",
      "Validation F1 Score: 0.802827965435978\n",
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize text data\n",
    "def tokenize_data(data, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in data:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Tokenize train and test data\n",
    "X_train_ids, X_train_masks = tokenize_data(train_data['text'].values)\n",
    "X_test_ids, X_test_masks = tokenize_data(test_data['text'].values)\n",
    "y_train = torch.tensor(train_data['target'].values)\n",
    "\n",
    "# Train-test split\n",
    "X_train_ids, X_val_ids, X_train_masks, X_val_masks, y_train, y_val = train_test_split(\n",
    "    X_train_ids, X_train_masks, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_ids, X_train_masks, y_train)\n",
    "val_dataset = TensorDataset(X_val_ids, X_val_masks, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(torch.long) if i < 2 else t.to(torch.float) for i, t in enumerate(batch))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        predictions, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                b_input_ids, b_input_mask, b_labels = tuple(t.to(torch.long) if i < 2 else t.to(torch.float) for i, t in enumerate(batch))\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                logits = outputs.logits\n",
    "                predictions.append(logits.cpu().numpy())\n",
    "                true_labels.append(b_labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_labels = np.concatenate(true_labels, axis=0)\n",
    "        predictions = np.where(predictions > 0.5, 1, 0)\n",
    "        val_f1 = f1_score(true_labels, predictions)\n",
    "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "        print(f\"Validation F1 Score: {val_f1}\")\n",
    "\n",
    "  \n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Predict on test data\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_data), 16):\n",
    "        b_input_ids = X_test_ids[i:i+16]\n",
    "        b_input_mask = X_test_masks[i:i+16]\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        test_predictions.append(logits.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "test_predictions = np.concatenate(test_predictions, axis=0).flatten()\n",
    "test_predictions = np.where(test_predictions > 0.5, 1, 0).astype(int)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})\n",
    "submission.to_csv('submission_3.csv', index=False)\n",
    "print(\"Submission file created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
